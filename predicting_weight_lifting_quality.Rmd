---
title: "Predicting Weight Lifting Quality"
author: "Wesley J. Silva"
date: "28 de abril de 2019"
output: html_document
params:
  fit_models: FALSE
---

```{r load_data, include = F}
library(data.table)
library(dplyr)
library(caret)
library(ggplot2)
library(gridExtra)
pml_data <- fread("data/pml-training.csv")
validate_testing <- fread("data/pml-testing.csv")
pml_data[,.N, by = "classe"]
```


# Introduction

In this exercise, the strategy is to predict the weight lifting quality using the original features, ignoring summarized measures at the end of a time window.

For predicting the classes, generalized boosting models (gbm) and random forest approaches were performed. The method with higher accuracy measured was the one selected to make the predictions in the test data.

```{r transform,include = F,results = 'hide'}
# window-user id's
window_id <- c("classe","user_name","raw_timestamp_part_1")

# euler_angle_vars
euler_angle_vars <- grep("^(pitch|yaw|roll)",names(pml_data),value = T)

# movement vars
  movement_vars <- grep("(x|y|z)$",names(pml_data),value = T)

# original features
orig_features <- c(euler_angle_vars,movement_vars)

transf_data <- function(data){
  if(!"classe" %in% names(data)) window_id <- c(window_id[-1],"V1")
  
  # selecting variables
  all_vars <- c(window_id,orig_features)
  data <- select(data,one_of(all_vars))
  
  # ordering data
  setorderv(data,cols = window_id)
  data

  # # getting averages
  # data_window_ag <-
  #   data[,lapply(.SD,mean,na.rm = T),by = c(window_id)] %>%
  #   setnames(orig_features,paste0("avg_",orig_features))
  # 
  # # # getting standard deviations
  # # data_window_ag <-
  # #   data_window_ag[
  # #     data[,lapply(.SD,sd,na.rm = T),by = c(window_id)],
  # #     on = window_id] %>%
  # #   setnames(orig_features,paste0("sd_",orig_features))
  # 
  # # getting min
  # data_window_ag <-
  #   data_window_ag[
  #     data[,lapply(.SD,min,na.rm = T),by = c(window_id)],
  #     on = window_id] %>%
  #   setnames(orig_features,paste0("min_",orig_features))
  #   
  #   # getting max
  #   data_window_ag <-
  #     data_window_ag[
  #       data[,lapply(.SD,max,na.rm = T),by = c(window_id)],
  #       on = window_id] %>%
  #     setnames(orig_features,paste0("max_",orig_features))
  #   
  #   # getting amplitude
  #   data_window_ag <-
  #     data_window_ag[
  #       data[,lapply(.SD,function(x){max(x,na.rm = T) - min(x,na.rm = T)}),
  #            by = c(window_id)],
  #       on = window_id] %>%
  #     setnames(orig_features,paste0("amplitude_",orig_features))
  #   
  #   # getting first observation
  #   data_window_ag <-
  #     data_window_ag[
  #       data[,lapply(.SD,head,n = 1),by = c(window_id)],
  #       on = window_id] %>%
  #     setnames(orig_features,paste0("first_",orig_features))
  #   
  #   # getting last observation
  #   data_window_ag <-
  #     data_window_ag[
  #       data[,lapply(.SD,tail,n = 1),by = c(window_id)],
  #       on = window_id] %>%
  #     setnames(orig_features,paste0("last_",orig_features))
  #   
  #   
  #   data_window_ag
  
}

# applying data transformation
pml_data_transf <- transf_data(pml_data)
validate_transf <-  transf_data(validate_testing)

# checking NAs
check_pml_data_na <- unlist(lapply(pml_data_transf,function(x)mean(is.na(x))))
check_testing_na <- unlist(lapply(validate_transf,function(x)mean(is.na(x))))
any(check_pml_data_na > 0)
any(check_testing_na > 0)

```

# Data partition

In order to evaluate the models, considering the validation ("test") data doesn't have the "class" varible, the 'training' data were splited in a training and data sets.

```{r partition,include = T,results = 'hide'}
#### test X training ----
set.seed(342)
inTrain <- createDataPartition(pml_data_transf$classe,p = 0.7, list = F)
training <- pml_data_transf[inTrain,]
testing <- pml_data_transf[-inTrain,]
```

# Feature selection

For reducing dimensionality, it was selected only the variables with relevant differences between groups. Therefore, an ANOVA was performed for each feature available in the dataset. 

```{r}
anova_tests <-
  lapply(select(training,-one_of(window_id)),
         function(x){
           anova(lm(x ~training$classe))$`Pr(>F)`[1]
         }) %>% 
  unlist
selected_features <- 
  names(anova_tests[anova_tests < 0.05])
```


# Descriptive analysis

The graphs below show some patterns observed for an arbitrary set of variables. 

```{r graphs, echo = F,results = 'asis'}
b_plot <- ggplot(training,aes(col = classe))

cat("\n\n The x-dimension for arm acceleration at the end of the window tends to be lower when the weight lifting is performed the right way.\n\n")
b_plot + geom_boxplot(aes(y = accel_arm_x))

cat("\n\nFurthermore, average belt acceleration in any direction seems to be a good predictor for class E exercises.\n\n")
b_plot + geom_boxplot(aes(y = accel_belt_y)) -> b1
b_plot + geom_boxplot(aes(y = accel_belt_x)) -> b2
b_plot + geom_boxplot(aes(y = accel_belt_z)) -> b3
grid.arrange(b1,b2,b3,ncol = 1)

cat("\n\n The amplitude of dumbbell's pitch seems to be higher in class B.\n\n")
b_plot + geom_boxplot(aes(y = pitch_dumbbell)) ## esse!

cat("\n\n Finally, the amplitue of forearm's roll tends to be smaller in the classes A and D, despite some outliers in class A.\n\n")
b_plot + geom_boxplot(aes(y = roll_forearm)) ## esse!

```


# Training the models

The predictors were trained using "gbm" and "rf" methods for boosting and random forest, respectively.

```{r train_models,include = T, eval = params$fit_models}
ti <- Sys.time()
set.seed(3676)
mod_boost <- train(classe~.,method = "gbm",
                   data = select(training,classe,
                                 one_of(selected_features)))
tf2 <- difftime(Sys.time(),ti)

save(mod_boost,file = "fitted_boost.rda")

ti <- Sys.time()
set.seed(3676)
mod_rf <- train(classe~.,method = "rf",
                   data = select(training,classe,
                                 one_of(selected_features)))
tf3 <- difftime(Sys.time(),ti)

# saving the models
save(mod_boost,mod_rf,file = "fitted_models.rda")

# saving the models
save(mod_boost,mod_rf,file = "fitted_models.rda")
```

```{r load_momdels,include = F,eval = !params$fit_models}
# loading the models
load(file = "fitted_models.rda")
```

# Evaluating on the separate test set

```{r confusion_matrix,include = F,results = 'hide'}
testing[,`:=`(pred_boost = predict(mod_boost,newdata = testing),
              pred_rf = predict(mod_rf,newdata = testing))]

tb_boost <-
  testing[,100*prop.table(table(Observed = classe,Predicted = pred_boost),1)]

tb_rf <- 
testing[,100*prop.table(table(Observed = classe,Predicted = pred_rf),2)]

# compares accuracy
comp_accuracy <- 
  c("boosting" = testing[,round(100*mean(pred_boost == classe),1)],
    "random forest" = testing[,round(100*mean(pred_rf == classe),1)])
better <- names(comp_accuracy[which.max(comp_accuracy)])

```

Using *boosting* algorithm, the estimated accuracy was `r comp_accuracy["boosting"]`%. On the other hand, random forest showed an accuracy of `r comp_accuracy["random forest"]`%, so `r better` is the model used for for predicting the classes.

The tables below show the confusion matrix of both classification, with lines representing the actual classes and the predictions coming in the columns. The percentages were calculated for each observed class, giving the sensitivity of each leval.

```{r confusion_matrix_p,echo = F,results = 'asis'}
knitr::kable(tb_boost,
             format.args = list(digits = 2))

knitr::kable(tb_rf,
             format.args = list(digits = 2))
```


```{r validate,include = F,results = 'hide'}
if (better == "boosting") mod_p <- mod_boost else mod_p <- mod_rf
validate_transf[,pred_class := predict(mod_p,
                                       newdata = validate_transf)]

View(validate_transf[,list(V1,pred_class)])
```

